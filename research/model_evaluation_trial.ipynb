{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\manob\\\\Documents\\\\Kidney_Disease_Detection\\\\Kidney_Disease_Detection'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class EvaluationConfig:\n",
    "    root_dir: Path\n",
    "    trained_model_path: Path\n",
    "    training_data: Path\n",
    "    params_image_size: list\n",
    "    params_batch_size: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manob\\Documents\\Kidney_Disease_Detection\\Kidney_Disease_Detection\\.conda\\Lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\manob\\Documents\\Kidney_Disease_Detection\\Kidney_Disease_Detection\\.conda\\Lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\manob\\Documents\\Kidney_Disease_Detection\\Kidney_Disease_Detection\\.conda\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from cnnClassifier.constants import *\n",
    "from cnnClassifier.utils.common import read_yaml, create_directories, save_json\n",
    "import tensorflow as tf\n",
    "from cnnClassifier.entity.config_entity import EvaluationConfig\n",
    "from cnnClassifier.logger import logger\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath=CONFIG_FILE_PATH,\n",
    "        params_filepath=PARAMS_FILE_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_evaluation_config(self) -> EvaluationConfig:\n",
    "        training_config = self.config.training\n",
    "        eval_config = self.config.model_evaluation\n",
    "\n",
    "        create_directories([Path(eval_config.root_dir)])\n",
    "\n",
    "        evaluation_config = EvaluationConfig(\n",
    "            root_dir=Path(eval_config.root_dir),\n",
    "            trained_model_path=Path(training_config.trained_model_path),\n",
    "            training_data=Path(training_config.training_data),\n",
    "            params_image_size=self.params.IMAGE_SIZE,\n",
    "            params_batch_size=self.params.BATCH_SIZE\n",
    "        )\n",
    "        return evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Class\n",
    "class Evaluation:\n",
    "    def __init__(self, config: EvaluationConfig):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.valid_generator = None\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Load the trained model\"\"\"\n",
    "        logger.info(f\"Loading trained model from {self.config.trained_model_path}\")\n",
    "        self.model = tf.keras.models.load_model(self.config.trained_model_path)\n",
    "        logger.info(\"Model loaded successfully\")\n",
    "\n",
    "    def create_validation_generator(self):\n",
    "        \"\"\"Create validation data generator\"\"\"\n",
    "        logger.info(\"Creating validation data generator...\")\n",
    "\n",
    "        datagenerator_kwargs = dict(\n",
    "            rescale=1./255,\n",
    "            validation_split=0.20\n",
    "        )\n",
    "\n",
    "        dataflow_kwargs = dict(\n",
    "            target_size=self.config.params_image_size[:-1],\n",
    "            batch_size=self.config.params_batch_size,\n",
    "            class_mode=\"binary\",\n",
    "            interpolation=\"bilinear\"\n",
    "        )\n",
    "\n",
    "        valid_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            **datagenerator_kwargs\n",
    "        )\n",
    "\n",
    "        # Use the combined training data directory created during training\n",
    "        combined_training_dir = \"artifacts/data_ingestion/Combined_Training_Data\"\n",
    "\n",
    "        self.valid_generator = valid_datagenerator.flow_from_directory(\n",
    "            directory=combined_training_dir,\n",
    "            subset=\"validation\",\n",
    "            shuffle=False,\n",
    "            **dataflow_kwargs\n",
    "        )\n",
    "        logger.info(f\"Validation generator created with {self.valid_generator.samples} samples\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate the model and log metrics\"\"\"\n",
    "        logger.info(\"Starting model evaluation...\")\n",
    "\n",
    "        # Ensure model and generator are initialized\n",
    "        if self.model is None:\n",
    "            self.load_model()\n",
    "        if self.valid_generator is None:\n",
    "            self.create_validation_generator()\n",
    "            \n",
    "        # Evaluate the model\n",
    "        scores = self.model.evaluate(self.valid_generator)\n",
    "        print(f\"Validation Loss: {scores[0]:.4f}\")\n",
    "        print.info(f\"Validation Accuracy: {scores[1]:.4f}\")\n",
    "        \n",
    "        # Generate predictions\n",
    "        predictions = self.model.predict(self.valid_generator)\n",
    "        y_pred = (predictions > 0.5).astype(int).flatten()\n",
    "        y_true = self.valid_generator.classes\n",
    "        \n",
    "        # Classification Report\n",
    "        report = classification_report(y_true, y_pred, target_names=['Non-Stone', 'Stone'])\n",
    "        logger.info(\"Classification Report:\\n\" + report)\n",
    "\n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        logger.info(\"Confusion Matrix:\\n\" + str(cm))\n",
    "\n",
    "        # Save evaluation metrics\n",
    "        metrics = {\n",
    "            \"loss\": float(scores[0]),\n",
    "            \"accuracy\": float(scores[1]),\n",
    "            \"classification_report\": report,\n",
    "            \"confusion_matrix\": cm.tolist()\n",
    "        }\n",
    "        save_path = self.config.root_dir / \"evaluation_metrics.json\"\n",
    "        with open(save_path, 'w') as f:\n",
    "            json.dump(metrics, f, indent=4)\n",
    "        logger.info(f\"Evaluation metrics saved to {save_path}\")\n",
    "\n",
    "    def save_model(self, path: Path):\n",
    "        \"\"\"Save the evaluated model if needed\"\"\"\n",
    "        self.model.save(path)\n",
    "        logger.info(f\"Model saved at {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-21 14:40:45,087: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-03-21 14:40:45,091: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-03-21 14:40:45,095: INFO: common: created directory at: artifacts]\n",
      "[2025-03-21 14:40:45,097: INFO: common: created directory at: artifacts\\evaluation]\n",
      "[2025-03-21 14:40:45,100: ERROR: 34234365: Error during evaluation: EvaluationConfig.__init__() got an unexpected keyword argument 'root_dir']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "EvaluationConfig.__init__() got an unexpected keyword argument 'root_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     10\u001b[39m     logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError during evaluation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      3\u001b[39m     config = ConfigurationManager()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     evaluation_config = \u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_evaluation_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     evaluation = Evaluation(config=evaluation_config)\n\u001b[32m      6\u001b[39m     evaluation.load_model()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mConfigurationManager.get_evaluation_config\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     13\u001b[39m eval_config = \u001b[38;5;28mself\u001b[39m.config.model_evaluation\n\u001b[32m     15\u001b[39m create_directories([Path(eval_config.root_dir)])\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m evaluation_config = \u001b[43mEvaluationConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrained_model_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrained_model_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams_image_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mIMAGE_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m evaluation_config\n",
      "\u001b[31mTypeError\u001b[39m: EvaluationConfig.__init__() got an unexpected keyword argument 'root_dir'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        config = ConfigurationManager()\n",
    "        evaluation_config = config.get_evaluation_config()\n",
    "        evaluation = Evaluation(config=evaluation_config)\n",
    "        evaluation.load_model()\n",
    "        evaluation.create_validation_generator()\n",
    "        evaluation.evaluate()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during evaluation: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
